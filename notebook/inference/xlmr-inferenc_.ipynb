{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee238ce-a554-4ef9-99ef-ad775b69b601",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ab6cb8-8a93-489e-a8cb-b99d0bf96fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "gpus    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04d702cf-80de-44b6-aa06-e77a4d5811a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf04a56b-9d3f-44bd-8c80-597d375af1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_addons as tfa \n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689abc2-59ea-441b-a35a-777a77fba8e4",
   "metadata": {},
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f922cd-d0a7-4417-b5f5-01694fb00f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 19:46:02.338750: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-01-31 19:46:02.338789: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (yitingtsai-G5-5590): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195115df-69aa-4fab-90d2-72c063bc5cc1",
   "metadata": {},
   "source": [
    "### Config / Set Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907b8d68-4e0b-439b-ba14-1dc1d2c905eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CSV_PATH = './data/test.csv'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 256\n",
    "MODEL_NAME = 'jplu/tf-xlm-roberta-base'\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bda1a4-5d4f-4c1b-bdd3-cfb598dab044",
   "metadata": {},
   "source": [
    "### Set Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119e4d5a-e44a-42f1-b0df-2f96c7d93ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6438c50-2ab4-4cbb-8363-ffbd49171190",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e704cb91-9233-47eb-97fd-92f5d8da4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set into Pandas Dataframe\n",
    "def load_test_data_into_dataframe(test_csv_location):\n",
    "    \"\"\"\n",
    "    Load CSV datasets into Pandas Dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    test_location : str \n",
    "        A string of path location of test dataset in csv format.\n",
    "        \n",
    "    Returns:\n",
    "    ------------\n",
    "    test_df : Pandas Dataframe\n",
    "        A Dataframe of test data.\n",
    "    \"\"\"\n",
    "    # Load csv in Pandas Dataframe\n",
    "    test_df = pd.read_csv(test_csv_location)\n",
    "    \n",
    "    return test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4b50b5-263d-44a6-aa84-edb11d562fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformer Tokenizer tokenize and encode the textual data into embedding\n",
    "def tokenizer_encode(texts, tokenizer, maxlen=512):\n",
    "    \"\"\"\n",
    "    Let Transformers Tokenizer API prepare the input data and encode, precisely tokenizing \n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    texts : list of str\n",
    "        A list of string to be encoded by the tokenizer.\n",
    "    tokenizer : Transformers AutoTokenizer\n",
    "        A Tensorflow AutoTokenizer object loaded in order to encode the text data.\n",
    "    max_len : int\n",
    "        An integer representing the maximun length of each sample, also as the shape of outputs from 'frozen' body of transformer model.\n",
    "        \n",
    "    Returns:\n",
    "    ------------\n",
    "    model : Numpy Array\n",
    "        An array of tokenizer-encoded vector from the texts.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        return_attention_mask=False, \n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    encoding_array = np.array(encoding['input_ids'])\n",
    "    \n",
    "    return encoding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4206559-69ef-449e-a4bb-0f17b7f1e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test set into Tensorflow Dataset API\n",
    "def load_test_into_tf_Dataset(tokenizer, test_df, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Load splitted test dataset into Tensorflow Dataset API for a more efficient input pipeline, especially for parallelism.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    tokenizer : Transformers AutoTokenizer\n",
    "        A Tensorflow AutoTokenizer object loaded in order to encode the text data.\n",
    "    test_df : Pandas Dataframe\n",
    "        A Dataframe of loaded test data.\n",
    "    batch_size : int\n",
    "        An integer indicating the size of the batch. Here uses 16*num_of_TPU_core (=128) by default.\n",
    "\n",
    "    \n",
    "    Returns \n",
    "    ------------\n",
    "    test_dataset : tf.data.Dataset\n",
    "        A Tensorflow Dataset API object of test set as an input pipeline for model inference.\n",
    "    \"\"\"\n",
    "    ## Tokenize the textual format data by calling tokenizer_encode()\n",
    "    x_test = tokenizer_encode(texts=test_df.content.values.tolist(), tokenizer=tokenizer, maxlen=MAX_LEN)\n",
    "    \n",
    "    ## Build Tensorflow Dataset objects\n",
    "    test_dataset = (\n",
    "        tf.data.Dataset\n",
    "        .from_tensor_slices(x_test)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "    \n",
    "    return test_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36a22d3-ed12-4f63-93c1-c63c01434ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build output layer on top of the transformer model\n",
    "def build_model(transformer, num_classes=3, activation='softmax', max_len=512):\n",
    "    \"\"\"\n",
    "    Create top layer on top of HuggingFace Transformer model for down-stream task. cls_token\n",
    "    In my case, a multi-class classification is the goal. Taking into account that there are 3 classes, \n",
    "    I use categorical accuracy, as well as weighted F1 score and Matthews correlation coefficient as metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    transformer : Transformers TFAutoModel\n",
    "        A string of path location of training dataset in csv format.\n",
    "    num_classes : int\n",
    "        A integer representing num\n",
    "    activation : str\n",
    "        A string indicating which actvation to be used in the output layer. \n",
    "    max_len : int\n",
    "        An integer representing the maximun length of each sample, also as the shape of outputs from 'frozen' body of transformer model.\n",
    "        \n",
    "    Returns:\n",
    "    ------------\n",
    "    model : \n",
    "        configed model ready to be used\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(units=num_classes, activation=activation, name=activation)(cls_token) # set units=3 because we have three classes\n",
    "    \n",
    "    # add weighted F1 score and Matthews correlation coefficient as metrics\n",
    "    f1 = tfa.metrics.F1Score(num_classes=num_classes, average='weighted')\n",
    "    mcc = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=num_classes)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['categorical_accuracy', f1, mcc])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18515f82-c57f-4448-a6c9-6e6a14ca4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights from fine-tuned model weights saved in directory\n",
    "def load_model(model_dir='./xlmr-model/', max_len=256):\n",
    "    \"\"\"\n",
    "    Function to load a keras model that uses a transformer layer\n",
    "    \n",
    "    Parameters :\n",
    "    ------------\n",
    "    model_dir : str\n",
    "        A string indicating where model's weight and config file are.\n",
    "    max_len : int\n",
    "        An integer representing the maximun length of each sample, to be passed to build_model() function.\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    model : \n",
    "        configed model with weights loaded from fine-tuned model.\n",
    "    \"\"\"\n",
    "    transformer = TFAutoModel.from_pretrained(model_dir)\n",
    "    model = build_model(transformer, max_len=max_len)\n",
    "    softmax = pickle.load(open(model_dir+'softmax.pickle', 'rb'))\n",
    "    model.get_layer('softmax').set_weights(softmax)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ce7e71c-08a2-4a2a-839b-320da60b2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to be used in later coonverting inference label from int back to string\n",
    "def label_int_2_str(x): \n",
    "    \"\"\"\n",
    "    Convert encoded int labels back to string sentiment labels.\n",
    "    \"\"\"\n",
    "    if x == 0:\n",
    "        return 'negative'\n",
    "    elif x == 1:\n",
    "        return 'neutral'\n",
    "    elif x == 2:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0302c4-7e98-4c3a-a8e2-b203e21ba446",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016e935-bb7d-4afd-af8d-abc7fa601f77",
   "metadata": {},
   "source": [
    "load test set into Pandas Dataframe and Tensorflow Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b6126c-00e7-41a9-83f7-a4fc8ff49e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = load_test_data_into_dataframe(TEST_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93602956-11e3-4b0b-8f2b-b24218718f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 19:48:46.968758: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_test_into_tf_Dataset(tokenizer=tokenizer, test_df=test_df, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff542285-c99e-42f9-879f-b4acf77fa14e",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5170575c-8325-49c0-b93e-64c344ce948f",
   "metadata": {},
   "source": [
    "load model weights from fine-tuned model weights saved in `/xlmr-model/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b69c50-5128-4e88-aea3-42ab55aabb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 19:49:52.130901: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "2022-01-31 19:49:52.903005: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "2022-01-31 19:49:52.998794: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "2022-01-31 19:49:55.597002: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "2022-01-31 19:49:56.229169: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 768006144 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at ./xlmr-model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n",
      "/home/yiting-tsai/synthesio/synthesio-test/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_dir='./xlmr-model/', max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb8b6fe-490a-4dd2-af40-d0531ddf55e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_word_ids (InputLayer)  [(None, 256)]            0         \n",
      "                                                                 \n",
      " tfxlm_roberta_model (TFXLMR  TFBaseModelOutputWithPoo  278043648\n",
      " obertaModel)                lingAndCrossAttentions(l            \n",
      "                             ast_hidden_state=(None,             \n",
      "                             256, 768),                          \n",
      "                              pooler_output=(None, 76            \n",
      "                             8),                                 \n",
      "                              past_key_values=None, h            \n",
      "                             idden_states=None, atten            \n",
      "                             tions=None, cross_attent            \n",
      "                             ions=None)                          \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 768)              0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 3)                 2307      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 278,045,955\n",
      "Trainable params: 278,045,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a374ad9c-8821-4e8c-9c82-105f961a8d77",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5104320f-d27f-468a-bb92-5907b3fee0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 660s 4s/step\n"
     ]
    }
   ],
   "source": [
    "prediction_array = model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f0fd5-f6d1-4ee7-8b30-a21fc6387c02",
   "metadata": {},
   "source": [
    "#### convert inference predictions back to sentiment labels in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e00d49b-7c4f-4bda-a04e-f0a7fee45687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>inference_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tudo ok, de acordo com o pedido</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This math below is quite true. The dexcom sens...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>its a stupid app. follow my advice and do not ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LASK könnte in Trondheim schon EL-Aufstieg feiern</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bagus...bagus..baguss....says mau pesan lg leb...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The SLO team is looking for Innovators. If you...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>السلعه غير مطابقة للصوره ولم تحتوي على بطارية</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I loved the look of this neck lace.  I was ver...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Waiting for the Doctorrrr - wee</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UUS LAHENDUS: teadlased võitlevad emakakaelavä...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content inference_sentiment\n",
       "0                    Tudo ok, de acordo com o pedido            positive\n",
       "1  This math below is quite true. The dexcom sens...             neutral\n",
       "2  its a stupid app. follow my advice and do not ...            negative\n",
       "3  LASK könnte in Trondheim schon EL-Aufstieg feiern             neutral\n",
       "4  bagus...bagus..baguss....says mau pesan lg leb...            positive\n",
       "5  The SLO team is looking for Innovators. If you...            positive\n",
       "6      السلعه غير مطابقة للصوره ولم تحتوي على بطارية            negative\n",
       "7  I loved the look of this neck lace.  I was ver...            positive\n",
       "8                   Waiting for the Doctorrrr - wee             negative\n",
       "9  UUS LAHENDUS: teadlased võitlevad emakakaelavä...             neutral"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert probability for each class to int label\n",
    "test_df['inference_int'] = np.argmax(prediction_array, axis=-1)\n",
    "# convert int label to string label : 0=negative, 1=neutral, 2=positive\n",
    "test_df['inference_sentiment'] = test_df['inference_int'].apply(label_int_2_str)\n",
    "# drop column of int label\n",
    "test_df = test_df.drop('inference_int', axis=1)\n",
    "# show 10 first rows / pairs of text comments and their corresponding predicted sentiment label\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8b725-ce63-4782-a0f3-1d65f2eba036",
   "metadata": {},
   "source": [
    "### Save Prediction to `csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6545442-728a-459f-8bd2-c3a267b6d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b5abd-6ea4-451a-bed3-085cd23830ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthesio-test",
   "language": "python",
   "name": "synthesio-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
