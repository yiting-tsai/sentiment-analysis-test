{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASER sentence embedding + DNN\n",
    "- goal : multi-class classification\n",
    "- labels : `positive`, `negative`, `neutral`\n",
    "- accelerator : GPU for sentence embedding and TPU v3-8 model training\n",
    "- training time : 4 minutes for creating sentence embedding on GPU with LASER and 6 minutes DNN training on TPU\n",
    "\n",
    "#### LASER Language-Agnostic SEntence Representations encoder\n",
    "> [FacebookResearch LASER](https://github.com/facebookresearch/LASER)\n",
    "> LASER encoder allow us to abstract away the language of the document by vectorizing the sentence with multilingual sentence embeddings. \n",
    "> It was trained on 93 languages in 23 different alphabets, from major languages to minor dialects.\n",
    "> LASER was trained on sentences of at least 4 words; shorter sentences or words will have degraded performance.\n",
    "\n",
    "\n",
    "#### model building process :\n",
    "Most processes are modularized into [functions](#hf). \n",
    "\n",
    "0. [import libraries, config TPU and set constant variables](#step0)\n",
    "1. [load datasets in dataframe](#step1) `load_data_into_dataframe(train_csv_location, test_csv_location)`\n",
    "2. [preprocess text to remove unwanted tokens such as @username or #hashtag](#step2) `preprocess_text(df)`\n",
    "3. [convert dependent variable (categorical) to one-hot-encoding](#step3) `encode_dependent_variable_in_OHE(train_df, label_name='sentiment')`\n",
    "4. [split training data further into training and validation set](#step4) `split_train_validation(train_df, X_columns_name, label, validation_size=0.15, random_state=42)`\n",
    "5. [let LASER make sentence encoding](#step5)  `laser_encode(text, lang='en', normalize=True)`\n",
    "6. Restart notebook and activate TPU, run all cells except the previous two on letting LASER make sentence embeddings and saving them, and load sentence embedding saved previsouly in `npy` format\n",
    "7. [create neural network model](#step7) `build_model(num_classes=3, activation='softmax')`\n",
    "8. [set `EarlyStopping` by monitoring validation loss to prevent overfitting and `LearningRateScheduler` to schedule adaptive learning rate to speed up training time and hopefully increase performance](#step8)\n",
    "9. [train model](#step9)\n",
    "10. [plot model performance after training](#step10) `plot_model_history(history, measures)`\n",
    "11. save model weights locally for inference\n",
    "\n",
    "\n",
    "\n",
    "Please refer to another notebook `data_analysis.ipynb` for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Libraries  <a class=\"anchor\" id=\"step0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:21.737202Z",
     "iopub.status.busy": "2022-02-01T02:07:21.736897Z",
     "iopub.status.idle": "2022-02-01T02:07:51.189296Z",
     "shell.execute_reply": "2022-02-01T02:07:51.188421Z",
     "shell.execute_reply.started": "2022-02-01T02:07:21.737170Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q laserembeddings laserembeddings[zh] laserembeddings[ja]\n",
    "!python -m laserembeddings download-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:51.192315Z",
     "iopub.status.busy": "2022-02-01T02:07:51.192006Z",
     "iopub.status.idle": "2022-02-01T02:07:58.191916Z",
     "shell.execute_reply": "2022-02-01T02:07:58.191021Z",
     "shell.execute_reply.started": "2022-02-01T02:07:51.192279Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import tensorflow_addons as tfa  \n",
    "from laserembeddings import Laser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:09.932328Z",
     "iopub.status.busy": "2022-02-01T02:09:09.931600Z",
     "iopub.status.idle": "2022-02-01T02:09:09.936529Z",
     "shell.execute_reply": "2022-02-01T02:09:09.935648Z",
     "shell.execute_reply.started": "2022-02-01T02:09:09.932295Z"
    }
   },
   "outputs": [],
   "source": [
    "# seeds\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# constand path\n",
    "TRAIN_CSV_PATH = '../input/technical-test/train.csv'\n",
    "TEST_CSV_PATH = '../input/technicaltest/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions <a class=\"anchor\" id=\"hf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.193608Z",
     "iopub.status.busy": "2022-02-01T02:07:58.193290Z",
     "iopub.status.idle": "2022-02-01T02:07:58.199844Z",
     "shell.execute_reply": "2022-02-01T02:07:58.198898Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.193576Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 1. load datasets in dataframe\n",
    "\n",
    "def load_data_into_dataframe(train_csv_location, test_csv_location):\n",
    "    \"\"\"\n",
    "    Load CSV datasets into Pandas Dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    train_location : str\n",
    "        A string of path location of training dataset in csv format.\n",
    "    test_location : str \n",
    "        A string of path location of test dataset in csv format.\n",
    "        \n",
    "    Returns:\n",
    "    ------------\n",
    "    train_df : Pandas Dataframe\n",
    "        A Dataframe of training data.\n",
    "    test_df : Pandas Dataframe\n",
    "        A Dataframe of test data.\n",
    "    \"\"\"\n",
    "    # Load csv in Pandas Dataframe\n",
    "    train_df = pd.read_csv(train_csv_location)\n",
    "    test_df = pd.read_csv(test_csv_location)\n",
    "    \n",
    "    return train_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.217830Z",
     "iopub.status.busy": "2022-02-01T02:07:58.217532Z",
     "iopub.status.idle": "2022-02-01T02:07:58.230298Z",
     "shell.execute_reply": "2022-02-01T02:07:58.229395Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.217792Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 2. preprocess text to get rid of unwanted tokens such as @username or #hashtag\n",
    "\n",
    "def preprocess_text(df):\n",
    "    \"\"\"\n",
    "    Preprocess texts in content column of training data to remove unwanted tokens.\n",
    "    \"\"\"\n",
    "    temp_df = pd.DataFrame(df.content.values)\n",
    "    texts = temp_df[temp_df.columns.values[0]]\n",
    "    texts = texts.apply(lambda s: re.sub('@\\w+', ' ', s))            # remove @usernames\n",
    "    texts = texts.apply(lambda s: re.sub('#',    ' ', s))            # remove hashtag prefixes    \n",
    "    texts = texts.apply(lambda s: re.sub('\\n',   ' ', s))            # remove newlines\n",
    "    texts = texts.apply(lambda s: re.sub('\\w+://\\S+',  '<URL>', s))  # remove urls    \n",
    "    texts = texts.apply(lambda s: re.sub('\\s+',  ' ', s))            # remove multiple spaces    \n",
    "    return list(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.202024Z",
     "iopub.status.busy": "2022-02-01T02:07:58.201703Z",
     "iopub.status.idle": "2022-02-01T02:07:58.216221Z",
     "shell.execute_reply": "2022-02-01T02:07:58.215457Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.201926Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 3. convert dependent variable (categorical) to one-hot-encoding\n",
    "\n",
    "def encode_dependent_variable_in_OHE(train_df, label_name='sentiment', num_classes=3):\n",
    "    \"\"\"\n",
    "    Encode the sentiment labels in the training Dataframe into one hot encoded format, since in the top layer output Dense layer use 'categorical_crossentropy'.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    train_df : Pandas Dataframe\n",
    "        A Dataframe of loaded training data.\n",
    "    label_name : str\n",
    "        A string of column name indicating the output variable in the train_df.\n",
    "    num_classes : int\n",
    "        An integer indicating how many categories there are.\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    dummy_y : Numpy Array\n",
    "        A matrix of one-hot-encoded class labels.\n",
    "    \"\"\"    \n",
    "    # get array of sentiment labels \n",
    "    Y = train_df[label_name].values\n",
    "    \n",
    "    # encode label values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Y)\n",
    "    encoded_Y = encoder.transform(Y)\n",
    "\n",
    "    # convert integers to dummy variables - one hot encoded\n",
    "    dummy_y = to_categorical(encoded_Y)\n",
    "    \n",
    "    return dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.231786Z",
     "iopub.status.busy": "2022-02-01T02:07:58.231546Z",
     "iopub.status.idle": "2022-02-01T02:07:58.247318Z",
     "shell.execute_reply": "2022-02-01T02:07:58.246561Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.231759Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 4. split training data further into training and validation set\n",
    "\n",
    "def split_train_validation(train_df, X_columns_name, label, validation_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Split training data into further training set and validation set in a ratio of ( 1-validation_size : validation_size).\n",
    "    In my case, I set 85% of the training data for training the model and 15% aside for validation.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    train_df : Pandas Dataframe\n",
    "        A Dataframe of loaded training data.\n",
    "    X_columns_name : str\n",
    "        A string of column name indicating the comments in texts in the train_df which will be used as independent variable for training.\n",
    "    label : Numpy Array or Tensorflow Tensor\n",
    "        An array of labels or a matrix of one-hot-encoded labels.\n",
    "    validation_size : float\n",
    "        A float number between 0-1 indicating desired validation size\n",
    "    random_state : int\n",
    "        Set the seed for reproucibility in splitting dataset.\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    train_texts : Numpy Array\n",
    "        An array of training set samples in textual format in the shape of (1-validation_size,).\n",
    "    val_texts : Numpy Array  \n",
    "        An array of validation set samples in textual format in the shape of (validation_size,).\n",
    "    y_train : Numpy Array  \n",
    "        An array of \n",
    "    y_valid : Numpy Array \n",
    "       \n",
    "    \"\"\"   \n",
    "    train_texts, val_texts, y_train, y_valid = train_test_split(train_df[X_columns_name].values, label, \n",
    "                                                  random_state=random_state, \n",
    "                                                  test_size=validation_size, shuffle=True)\n",
    "    \n",
    "    print('Total number of examples: ', len(train_df))\n",
    "    print('number of training set examples: ', len(train_texts))\n",
    "    print('number of validation set examples: ', len(val_texts))\n",
    "    \n",
    "    return train_texts, val_texts, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.249700Z",
     "iopub.status.busy": "2022-02-01T02:07:58.248916Z",
     "iopub.status.idle": "2022-02-01T02:07:58.263898Z",
     "shell.execute_reply": "2022-02-01T02:07:58.263012Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.249654Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 5. Build LASER sentence embedding model to encode.\n",
    "# based on https://www.kaggle.com/jamesmcguigan/nlp-laser-embeddings-keras\n",
    "# normalize or not ? https://stats.stackexchange.com/questions/177905/should-i-normalize-word2vecs-word-vectors-before-using-them\n",
    "\n",
    "def laser_encode(text, lang='en', normalize=True):\n",
    "    \"\"\"\n",
    "    Build LASER embedding model to get ready to encode\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    text : str or list of str\n",
    "        A string or a list of string to be encoded by LASER.\n",
    "    lang : str\n",
    "        A string to specify which language to set the tokenizer, that string shall be in ISO 639-1 form.\n",
    "    normalize : bool\n",
    "        Default to True. Classification cares about the direction of vectors \n",
    "        in order to solve relations between vecotrs, so set normalize=True is more logical.\n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    embedding : Numpy Array\n",
    "        A matrix of encoded sentences in shape of (len([text]), 1024)\n",
    "    \"\"\"\n",
    "    laser = Laser()\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        sentences = [ text ]\n",
    "    else:\n",
    "        sentences = list(text)\n",
    "\n",
    "    embedding = laser.embed_sentences(sentences, lang=lang)\n",
    "    \n",
    "    if normalize:\n",
    "        embedding = embedding / np.sqrt(np.sum(embedding**2, axis=1)).reshape(-1,1)\n",
    "        \n",
    "    return embedding    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.265401Z",
     "iopub.status.busy": "2022-02-01T02:07:58.265128Z",
     "iopub.status.idle": "2022-02-01T02:07:58.290485Z",
     "shell.execute_reply": "2022-02-01T02:07:58.289200Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.265354Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 7. create neural network model\n",
    "\n",
    "def build_model(num_classes=3, activation='softmax'):\n",
    "    \"\"\"\n",
    "    Create top layer on top of HuggingFace Transformer model for down-stream task. cls_token\n",
    "    In my case, a multi-class classification is the goal. Taking into account that there are 3 classes, \n",
    "    I use categorical accuracy, as well as weighted F1 score and Matthews correlation coefficient as metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    num_classes : int\n",
    "        A integer representing num\n",
    "    activation : str\n",
    "        A string indicating which actvation to be used in the output layer. \n",
    "    \n",
    "    Returns:\n",
    "    ------------\n",
    "    model : \n",
    "        configed model ready to be train\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(1024,)),\n",
    "        Dense(units=512, activation=LeakyReLU(alpha=0.1)),\n",
    "        Dense(units=256, activation=LeakyReLU(alpha=0.1)),\n",
    "        Dropout(0.1),\n",
    "        Dense(units=64, activation=LeakyReLU(alpha=0.1)),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=32, activation=LeakyReLU(alpha=0.1)),\n",
    "        Dense(units=num_classes, activation=activation, name='softmax')])\n",
    "    \n",
    "    # add weighted F1 score and Matthews correlation coefficient as metrics\n",
    "    f1 = tfa.metrics.F1Score(num_classes=num_classes, average='weighted')\n",
    "    mcc = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=num_classes)\n",
    "    \n",
    "    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['categorical_accuracy', f1, mcc])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:07:58.293033Z",
     "iopub.status.busy": "2022-02-01T02:07:58.292141Z",
     "iopub.status.idle": "2022-02-01T02:07:58.302234Z",
     "shell.execute_reply": "2022-02-01T02:07:58.301288Z",
     "shell.execute_reply.started": "2022-02-01T02:07:58.292962Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 10. plot model performance after training\n",
    "\n",
    "def plot_model_history(history, measures):\n",
    "    \"\"\"\n",
    "    Plot history for visualization of performance measures in matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    history : Keras History object\n",
    "        A History object outputted from model.fit\n",
    "    measure : str list \n",
    "        A list of string of which performance measures to be visualized    \n",
    "    \"\"\"\n",
    "    for measure in measures:\n",
    "        plt.plot(history.history[measure])\n",
    "        plt.plot(history.history['val_' + measure])\n",
    "        plt.title('model performance : ' + measure.replace(\"_\", \" \"))\n",
    "        plt.ylabel(measure.replace(\"_\", \" \"))\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load datasets in dataframe <a class=\"anchor\" id=\"step1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:15.964748Z",
     "iopub.status.busy": "2022-02-01T02:09:15.964093Z",
     "iopub.status.idle": "2022-02-01T02:09:16.223923Z",
     "shell.execute_reply": "2022-02-01T02:09:16.223108Z",
     "shell.execute_reply.started": "2022-02-01T02:09:15.964713Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = load_data_into_dataframe(TRAIN_CSV_PATH, TEST_CSV_PATH)\n",
    "# print(train_df.shape[0]) #--> 25000\n",
    "# print(test_df.shape[0]) #--> 2500\n",
    "\n",
    "# train_df.sentiment.value_counts()\n",
    "# train_df.sentiment.value_counts() / # train_df.sentiment.value_counts().sum()\n",
    "\n",
    "# remove the one example with 'unassigned' label (data analysis is done beforehand, so I'll just proceed to delete this example here)\n",
    "train_df = train_df.drop(train_df[train_df.sentiment == 'unassigned'].index)\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess text to remove unwanted tokens such as @username or #hashtag <a class=\"anchor\" id=\"step2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:17.849151Z",
     "iopub.status.busy": "2022-02-01T02:09:17.848830Z",
     "iopub.status.idle": "2022-02-01T02:09:18.600773Z",
     "shell.execute_reply": "2022-02-01T02:09:18.600099Z",
     "shell.execute_reply.started": "2022-02-01T02:09:17.849123Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_content = preprocess_text(train_df)\n",
    "train_df['preprocessed_content'] = preprocessed_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert dependent variable (categorical) to one-hot-encoding <a class=\"anchor\" id=\"step3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:18.602363Z",
     "iopub.status.busy": "2022-02-01T02:09:18.602107Z",
     "iopub.status.idle": "2022-02-01T02:09:18.616609Z",
     "shell.execute_reply": "2022-02-01T02:09:18.615581Z",
     "shell.execute_reply.started": "2022-02-01T02:09:18.602333Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe_y = encode_dependent_variable_in_OHE(train_df, label_name='sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split training data further into training and validation set <a class=\"anchor\" id=\"step4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:20.255637Z",
     "iopub.status.busy": "2022-02-01T02:09:20.255362Z",
     "iopub.status.idle": "2022-02-01T02:09:20.267919Z",
     "shell.execute_reply": "2022-02-01T02:09:20.266810Z",
     "shell.execute_reply.started": "2022-02-01T02:09:20.255610Z"
    }
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, y_train, y_val = split_train_validation(train_df, \n",
    "                                                                X_columns_name='preprocessed_content', \n",
    "                                                                label=ohe_y, \n",
    "                                                                validation_size=0.15, \n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Let LASER make sentence encoding  <a class=\"anchor\" id=\"step5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save sentence embedding in `npy` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T01:51:25.801486Z",
     "iopub.status.busy": "2022-02-01T01:51:25.801001Z",
     "iopub.status.idle": "2022-02-01T01:52:11.481815Z",
     "shell.execute_reply": "2022-02-01T01:52:11.480686Z",
     "shell.execute_reply.started": "2022-02-01T01:51:25.801444Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# use GPU for speed up LASER embedding\n",
    "x_train = laser_encode(train_texts.tolist(), lang='en', normalize=True)\n",
    "x_val = laser_encode(val_texts.tolist(), lang='en', normalize=True)\n",
    "\n",
    "with open('train.npy', 'wb') as f1:\n",
    "    np.save(f1, x_train)\n",
    "f1.close()    \n",
    "with open('test.npy', 'wb') as f2:\n",
    "    np.save(f2, x_val)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T01:52:33.089460Z",
     "iopub.status.busy": "2022-02-01T01:52:33.089084Z",
     "iopub.status.idle": "2022-02-01T01:52:33.101131Z",
     "shell.execute_reply": "2022-02-01T01:52:33.100483Z",
     "shell.execute_reply.started": "2022-02-01T01:52:33.089389Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Restart notebook and activate TPU, run all cells except the previous two on letting LASER make sentence embeddings and saving them, and load sentence embedding saved previsouly in `npy` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:23.655440Z",
     "iopub.status.busy": "2022-02-01T02:09:23.654786Z",
     "iopub.status.idle": "2022-02-01T02:09:26.315716Z",
     "shell.execute_reply": "2022-02-01T02:09:26.314629Z",
     "shell.execute_reply.started": "2022-02-01T02:09:23.655398Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../input/laser-embedding/train.npy', 'rb') as f1:\n",
    "    x_train = np.load(f1)\n",
    "f1.close()    \n",
    "with open('../input/laser-embedding/test.npy', 'rb') as f2:    \n",
    "    x_val = np.load(f2)\n",
    "f1.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. TPU Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:26.317684Z",
     "iopub.status.busy": "2022-02-01T02:09:26.317456Z",
     "iopub.status.idle": "2022-02-01T02:09:31.988958Z",
     "shell.execute_reply": "2022-02-01T02:09:31.987998Z",
     "shell.execute_reply.started": "2022-02-01T02:09:26.317659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Create neural network model <a class=\"anchor\" id=\"step7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:31.998505Z",
     "iopub.status.busy": "2022-02-01T02:09:31.998225Z",
     "iopub.status.idle": "2022-02-01T02:09:32.612884Z",
     "shell.execute_reply": "2022-02-01T02:09:32.612064Z",
     "shell.execute_reply.started": "2022-02-01T02:09:31.998470Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    model = build_model(num_classes=3, activation='softmax')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Set `EarlyStopping` by monitoring validation loss to prevent overfitting and `LearningRateScheduler` to schedule adaptive learning rate to speed up training time and hopefully increase performance <a class=\"anchor\" id=\"step8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:31.990332Z",
     "iopub.status.busy": "2022-02-01T02:09:31.990111Z",
     "iopub.status.idle": "2022-02-01T02:09:31.996564Z",
     "shell.execute_reply": "2022-02-01T02:09:31.995500Z",
     "shell.execute_reply.started": "2022-02-01T02:09:31.990307Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "ES_callback = EarlyStopping(monitor='val_loss', patience=3, mode='auto')\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 60:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "LR_callback = LearningRateScheduler(scheduler) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Train model <a class=\"anchor\" id=\"step8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:09:32.614640Z",
     "iopub.status.busy": "2022-02-01T02:09:32.614321Z",
     "iopub.status.idle": "2022-02-01T02:16:00.757623Z",
     "shell.execute_reply": "2022-02-01T02:16:00.756514Z",
     "shell.execute_reply.started": "2022-02-01T02:09:32.614602Z"
    }
   },
   "outputs": [],
   "source": [
    "n_steps = train_texts.shape[0] // BATCH_SIZE\n",
    "\n",
    "train_history = model.fit(\n",
    "    x_train, y_train,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[TqdmCallback(verbose=2), ES_callback, LR_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Plot model performance after training <a class=\"anchor\" id=\"step10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:16:40.561493Z",
     "iopub.status.busy": "2022-02-01T02:16:40.560779Z",
     "iopub.status.idle": "2022-02-01T02:16:41.253503Z",
     "shell.execute_reply": "2022-02-01T02:16:41.252654Z",
     "shell.execute_reply.started": "2022-02-01T02:16:40.561449Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_history(train_history, ['categorical_accuracy', 'loss', 'f1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Save model weights locally for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:17:35.185824Z",
     "iopub.status.busy": "2022-02-01T02:17:35.185332Z",
     "iopub.status.idle": "2022-02-01T02:17:35.387845Z",
     "shell.execute_reply": "2022-02-01T02:17:35.387135Z",
     "shell.execute_reply.started": "2022-02-01T02:17:35.185791Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('laser_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:17:42.104943Z",
     "iopub.status.busy": "2022-02-01T02:17:42.103802Z",
     "iopub.status.idle": "2022-02-01T02:17:42.922112Z",
     "shell.execute_reply": "2022-02-01T02:17:42.921040Z",
     "shell.execute_reply.started": "2022-02-01T02:17:42.104859Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T02:17:55.543254Z",
     "iopub.status.busy": "2022-02-01T02:17:55.542846Z",
     "iopub.status.idle": "2022-02-01T02:17:55.550822Z",
     "shell.execute_reply": "2022-02-01T02:17:55.550131Z",
     "shell.execute_reply.started": "2022-02-01T02:17:55.543216Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('laser_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
